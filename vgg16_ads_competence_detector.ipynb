{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michellethr/candy/blob/main/vgg16_ads_competence_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqEv1Uyz-u76"
      },
      "source": [
        "# Convolutional Neural Networks: Candy Crush Competence Ads\n",
        "\n",
        "## Final project for the neural networks module at CodeOp's data science bootcamp\n",
        "\n",
        "The aim of this project is to train a convolutional neural network that will learn to distinguish between match-3 games and non-match-3 games. An adapted artificial intelligence such as this could be applied to any app, to detect ads that belong to the competence!\n",
        "\n",
        "Deep learning is a subset of machine learning algorithm that can learn automatically from complex input. There is a so-called \"black box\" with \"hidden layers\" that takes input and runs a series of very complicated processes known as multi-layer neural networks. Humans have developed such a smart machine to solve data problems that intail images, sounds and other multidimensional data.\n",
        "\n",
        "In this project, a convolutional neural network (CNN) will receive thousands of ad screenshots and train through a *asdfgh* architecture (our black box) and test its predictions on what is being advertised with another thousand of ad screenshots that has never seen before. This process will be repeated several times, changing the data for the training and the testing and checking how different are its predictions from the actual answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iME2BMT2Dc83"
      },
      "source": [
        "## Since deep learning requires large amounts of memory, we will load the data and build and execute our model on Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQVMgDvXDGGP",
        "outputId": "9389183d-7cd1-4cb4-be2d-63dc3675da03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhp8h1l47ytV"
      },
      "source": [
        "## Similar to cameras resembling how our eyes process images into our brain, these next lines of code will transform the images into a compatible format for our convolutional neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7xUE3_0EYkK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "ad_cats = ['competence', 'non-competence']\n",
        "dataset = []\n",
        "\n",
        "for cat in ad_cats:\n",
        "    folder = os.path.join('/content/gdrive/MyDrive/ADS', cat)\n",
        "    for ad_file in os.listdir(folder):\n",
        "        ad_path = os.path.join(folder, ad_file)\n",
        "\n",
        "        img = Image.open(ad_path)\n",
        "        img_tensor = torch.tensor(np.rollaxis(np.array(img), -1), dtype=torch.float32)\n",
        "\n",
        "        label = torch.zeros(2)\n",
        "        label[ad_cats.index(cat)] = 1\n",
        "\n",
        "        dataset.append((img_tensor, label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cipb5qIIChS"
      },
      "source": [
        "# The architecture of our neural network model: each line under _def init_  is a hidden layer and each line under _def forward_ is an image going through every one of the layers in a specific order.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHMLi13WHpZX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvolutionalNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvolutionalNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(4)\n",
        "\n",
        "        self.linear_1 = nn.Linear(1024, 16)\n",
        "        self.linear_2 = nn.Linear(16, 2)\n",
        "\n",
        "        self.batch_norm_1 = nn.BatchNorm2d(16)\n",
        "        self.batch_norm_2 = nn.BatchNorm2d(32)\n",
        "        self.batch_norm_3 = nn.BatchNorm2d(64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.batch_norm_1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.batch_norm_2(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = self.batch_norm_3(x)\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = F.relu(self.linear_1(x.view(-1, 1024)))\n",
        "        x = self.linear_2(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def get_number_of_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "# This specific network structure is called VGG-16 and was  was developed by Simonyan and Zisserman in 2014. So kudos to him."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr848ZBR9oS8"
      },
      "source": [
        "## As any model, we need to divide our data to not only learn to make predictions but ensure that the model will be able to predict on new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyCIqYRC6RCb"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfRYwup-njqj"
      },
      "source": [
        "## Before running this neural network we have to arrange quite a few things. First, we have to initialize the CNN and define the loaders that will itirate through the data. The train loader will feed the model 32 random batches of images for every training round, while this will feed the same testing singular batch of images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWNyV7W5nkIm"
      },
      "outputs": [],
      "source": [
        "model = ConvolutionalNN()\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etr6NeCVqlV3"
      },
      "source": [
        "## Secondly, we define our loss function and optimizer. The loss function measures how many predictions the model got wrong in every round, Cross Entropy is the standard for image classification tasks. The optimizer will readjust the model's parameters in every round to find ones that result in the highest accuracy scores, these values are also standard practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAoIHrE2qlw7"
      },
      "outputs": [],
      "source": [
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "model = ConvolutionalNN()\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
        "\n",
        "# Move the model to the device\n",
        "device = 'cpu'\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdLepsDhoRXa"
      },
      "source": [
        "## Now we can finally train the CNN with our data, that is the ads and their corresponding labels (competence and non-competence). It is common to create a loop that iterates the model a certain amount of times (epochs) and tracks the evaluation measures (loss and accuracy) on how well it does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRVPc1KAs9zl",
        "outputId": "df2d317d-c76e-49ac-be87-cb3b2d22913c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: training loss: 1.356, test loss: 1.351, test accuracy: 43.48%\n",
            "          \n",
            "Epoch 2: training loss: 1.314, test loss: 1.326, test accuracy: 69.57%\n",
            "          \n",
            "Epoch 3: training loss: 1.263, test loss: 1.311, test accuracy: 65.22%\n",
            "          \n",
            "Epoch 4: training loss: 1.231, test loss: 1.318, test accuracy: 58.70%\n",
            "          \n",
            "Epoch 5: training loss: 1.197, test loss: 1.307, test accuracy: 69.57%\n",
            "          \n",
            "Epoch 6: training loss: 1.172, test loss: 1.309, test accuracy: 65.22%\n",
            "          \n",
            "Epoch 7: training loss: 1.176, test loss: 1.308, test accuracy: 67.39%\n",
            "          \n",
            "Epoch 8: training loss: 1.147, test loss: 1.313, test accuracy: 67.39%\n",
            "          \n",
            "Epoch 9: training loss: 1.134, test loss: 1.335, test accuracy: 63.04%\n",
            "          \n",
            "Epoch 10: training loss: 1.119, test loss: 1.297, test accuracy: 71.74%\n",
            "          \n",
            "Epoch 11: training loss: 1.114, test loss: 1.307, test accuracy: 65.22%\n",
            "          \n",
            "Epoch 12: training loss: 1.099, test loss: 1.335, test accuracy: 69.57%\n",
            "          \n",
            "Epoch 13: training loss: 1.101, test loss: 1.329, test accuracy: 69.57%\n",
            "          \n",
            "Epoch 14: training loss: 1.098, test loss: 1.319, test accuracy: 71.74%\n",
            "          \n",
            "Epoch 15: training loss: 1.080, test loss: 1.330, test accuracy: 71.74%\n",
            "          \n",
            "Epoch 16: training loss: 1.074, test loss: 1.307, test accuracy: 71.74%\n",
            "          \n",
            "Epoch 17: training loss: 1.074, test loss: 1.347, test accuracy: 67.39%\n",
            "          \n",
            "Epoch 18: training loss: 1.075, test loss: 1.355, test accuracy: 67.39%\n",
            "          \n",
            "Epoch 19: training loss: 1.053, test loss: 1.343, test accuracy: 69.57%\n",
            "          \n",
            "Epoch 20: training loss: 1.050, test loss: 1.357, test accuracy: 69.57%\n",
            "          \n",
            "Epoch 21: training loss: 1.051, test loss: 1.372, test accuracy: 69.57%\n",
            "          \n",
            "Epoch 22: training loss: 1.043, test loss: 1.317, test accuracy: 73.91%\n",
            "          \n",
            "Epoch 23: training loss: 1.054, test loss: 1.375, test accuracy: 73.91%\n",
            "          \n",
            "Epoch 24: training loss: 1.061, test loss: 1.457, test accuracy: 65.22%\n",
            "          \n"
          ]
        }
      ],
      "source": [
        "epochs = 60\n",
        "n_samples_train = len(train_loader)\n",
        "n_samples_test = len(test_loader)\n",
        "training_loss_per_epoch, test_loss_per_epoch, test_accuracy_per_epoch = [], [], []\n",
        "best_accuracy = - torch.inf\n",
        "for epoch in range(epochs):\n",
        "    training_loss = 0\n",
        "    for data, labels in train_loader:\n",
        "        predict = model(data.to(device))\n",
        "        loss = loss_function(labels.to(device), predict)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        training_loss += (loss.item() / n_samples_train)\n",
        "    test_loss, correct, total = (0, 0, 0)\n",
        "    for data, label in test_loader:\n",
        "        predict = model(data.to(device))\n",
        "        loss = loss_function(label.to(device), predict)\n",
        "        test_loss += (loss.item() / n_samples_test)\n",
        "        if torch.argmax(predict) == torch.argmax(label):\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    accuracy = correct/total*100\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "    training_loss_per_epoch.append(training_loss)\n",
        "    test_loss_per_epoch.append(test_loss)\n",
        "    test_accuracy_per_epoch.append(accuracy)\n",
        "    print('''Epoch {}: training loss: {:.3f}, test loss: {:.3f}, test accuracy: {:.2f}%\n",
        "          '''.format(epoch+1, training_loss, test_loss, accuracy))\n",
        "print(f'\\n Best test accuracy: {best_accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFiWPizytlGe"
      },
      "source": [
        "## Can you see how the loss goes down while the accuracy goes up? The best this model can do at the moment is to detect 86% of the competence ads that sneak in our customers screens among all kinds of games."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHhtei2_wK0G"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
        "ax[0].plot(test_loss_per_epoch, c='r')\n",
        "ax[0].set_xlabel('Epochs', fontsize=16)\n",
        "ax[0].set_ylabel('Loss', fontsize=16)\n",
        "ax[1].plot(test_accuracy_per_epoch, c='g')\n",
        "ax[1].set_xlabel('Epochs', fontsize=16)\n",
        "ax[1].set_ylabel('Accuracy %', fontsize=16)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}